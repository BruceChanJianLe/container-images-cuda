# - DO NOT EDIT - FILE IS AUTOGENERATED
image: docker:stable

# NOTE: The docker:stable-dind service is not used here because --add-runtime=nvidia does not work from DIND and we need the
#       runtime to perform tests

# For multi-arch pipeline stages on x86_64, qemu-user-static and systemd are required with the following configuration:
#
# $ cat /etc/binfmt.d/qemu-static.conf
# :qemu-aarch64:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\xb7:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff:/usr/bin/qemu-aarch64-static:CF
# :qemu-ppc64le:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x15\x00:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff\x00:/usr/bin/qemu-ppc64le-static:CF
#
# < reboot or start systemd-binfmt.service >
#
# Check with (on x86_64):
#
# $ systemctl status systemd-binfmt.service
# < service should be started >
#
# $ docker run -it nvidia/cuda-ppc64le:11.0-base-ubuntu18.04-rc
# < container should run>

variables:
  # Need a value of two here for checking the manifest in the last commit
  GIT_DEPTH: "2"
  IMAGE_NAME: "nvidia/cuda"

before_script:
  # WARNING: DO NOT install qemu-${ARCH} alpine package here for use in qemu-user-static multi-arch container builds, there
  # WARNING: is a bug with DNS that causes it to fail with the internal nvidia domains (cuda-repo.nvidia.com)
  # WARNING: SEE: https://gitlab.alpinelinux.org/alpine/aports/issues/8131
  - apk add --no-cache git bash findutils python3 python3-dev curl g++ libmagic
  - python3 -m ensurepip && \
  - rm -r /usr/lib/python*/ensurepip && \
  - pip3 install --upgrade pip setuptools && \
  - if [ ! -e /usr/bin/pip ]; then ln -s pip3 /usr/bin/pip ; fi && \
  - if [[ ! -e /usr/bin/python ]]; then ln -sf /usr/bin/python3 /usr/bin/python; fi && \
  - curl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python
  - source $HOME/.poetry/env
  - poetry config virtualenvs.create false && poetry install
  - export OS="$(echo ${CI_JOB_NAME} | cut -f1 -d-)"
  - export OS_NAME="$(echo ${CI_JOB_NAME} | cut -f1 -d-| sed 's/[[:digit:]\.]*//g')"
  - export OS_VERSION="$(echo ${CI_JOB_NAME} | cut -f1 -d- | sed -n 's/[[:alpha:]]\+\([[:digit:]\.]\+\)/\1/p')"
  - export CUDA_VERSION="$(echo ${CI_JOB_NAME} | cut -f2 -d- | awk -Fv '{ print $2 }')"
  - export CUDNN_VERSION="$(echo ${CI_JOB_NAME} | cut -f3 -d-)"
  - if [[ ! -z ${ARCH} ]] && [[ "${ARCH}" != "x86_64" ]]; then
      if [[ ${ARCH} == "ppc64le" ]]; then
        export IMAGE_NAME="nvidia/cuda-ppc64le";
      else
        export IMAGE_NAME="nvidia/cuda-arm64";
      fi
    fi
  - 'echo "ARCH: $ARCH"'
  - 'echo "OS: $OS"'
  - 'echo "IMAGE_NAME: $IMAGE_NAME"'
  - 'echo "OS_NAME: $OS_NAME"'
  - 'echo "OS_VERSION: $OS_VERSION"'
  - 'echo "CUDA_VERSION: $CUDA_VERSION"'
  - 'echo "CUDNN_VERSION: $CUDNN_VERSION"'
  - 'echo "IMAGE_TAG_SUFFIX: $IMAGE_TAG_SUFFIX"'
  - 'echo "LATEST: $LATEST"'
  - 'echo "NO_OS_SUFFIX: $NO_OS_SUFFIX"'

stages:
  # trigger stage is a workaround until https://gitlab.com/gitlab-org/gitlab-ce/issues/22972 is implemented
  - trigger
  - cuda
  - cudnn
  - test
  - scan
  - deploy

.tags_template: &tags_definition
  tags:
    - cuda-docker

# Only used for CUDA 8.0, will be removed once CUDA 8 support is dropped
.cuda_template_depricated: &cuda_definition_deprecated
  stage: cuda
  <<: *tags_definition
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel ";
      fi
    - if [[ "${LATEST}" == "true" ]]; then
        export TAG_LATEST=" -t ${IMAGE_NAME}:latest ";
      fi
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}" ${TAG_RUNTIME} ${TAG_LATEST} "dist/${OS}/${CUDA_VERSION}/runtime"
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}" ${TAG_DEVEL} --build-arg "IMAGE_NAME=${IMAGE_NAME}" "dist/${OS}/${CUDA_VERSION}/devel"

.cuda_base_template: &cuda_base_definition
  stage: cuda
  <<: *tags_definition
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_BASE=" -t ${IMAGE_NAME}:${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} ";
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
      fi
    - if [[ "${LATEST}" == "true" ]]; then
        export TAG_LATEST=" -t ${IMAGE_NAME}:latest ";
      fi
    - if [[ -z $DIST_BASE_PATH ]]; then
        export DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    # See https://github.com/multiarch/qemu-user-static for options;
    # It is possible to overide the qemu-${ARCH} binary by moving the following line to after the docker run
    # This will use the qemu-${ARCH} binary installed in the the docker gitlab-runner alpine image "docker:stable"
    # cp /usr/bin/qemu-${ARCH} ${DIST_BASE_PATH}/${OS}/${CUDA_VERSION}/${ARCH}/base/;
    - if [[ "${ARCH}" != "x86_64" ]]; then
        docker run --rm --privileged multiarch/qemu-user-static:register;
      fi
    - 'echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"'
    - 'echo "TAG_LATEST: ${TAG_LATEST}"'
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_BASE} "${DIST_BASE_PATH}/base"
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_RUNTIME} --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime"
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_DEVEL} ${TAG_LATEST} --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel"

.cudnn_template: &cudnn_definition
  stage: cudnn
  <<: *tags_definition
  script:
    - if [[ -z $DIST_BASE_PATH ]]; then
        DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - 'echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"'
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        TAG_CUDNN_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
        TAG_CUDNN_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
      fi
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_RUNTIME}
                   --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime/${CUDNN_VERSION}"
    - docker build -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_DEVEL}
                   --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel/${CUDNN_VERSION}"

.test_template: &test_definition
  stage: test
  <<: *tags_definition
  script:
    - bash -e ./test/scripts/bats_install.sh
    - bash -e ./test/scripts/run_tests.sh

.scan_template: &scan_definition
  stage: scan
  <<: *tags_definition
  script:
    - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/sectooling/scanning/contamer.git
    - cd contamer
    - pip3 install -r requirements.txt
    - python3 contamer.py --external-policy -ls ${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}
    - export CONTAMER_RET=$?
    - >-
      if [[ $CONTAMER_RET -ne 0 ]]; then
        exit 1;
      else
        echo "Contamer return value: ${CONTAMER_RET}";
      fi

.deploy_template: &deploy_definition
  stage: deploy
  <<: *tags_definition
  script:
    - python manager.py ${MANIFEST:+`echo "--manifest ${MANIFEST}"`} push
        --image-name "${IMAGE_NAME}"
        --os "${OS_NAME}"
        --os-version "${OS_VERSION}"
        --cuda-version "${CUDA_VERSION}"
        --arch "${ARCH}"
        ${IMAGE_TAG_SUFFIX:+`echo "--tag-suffix ${IMAGE_TAG_SUFFIX}"`} ${DRY_RUN:+"-n"}

trigger:
  <<: *tags_definition
  variables:
    MANIFEST: "{{ cuda.manifest_path }}"
  script:
    - echo CI_COMMIT_MESSAGE:$CI_COMMIT_MESSAGE
    - python manager.py ${MANIFEST:+`echo "--manifest ${MANIFEST}"`} ${TRIGGER_OVERRIDE:+`echo "--trigger-override ${TRIGGER_OVERRIDE}"`} trigger
  only:
    variables:
      - $TRIGGER == null
      - $TRIGGER_OVERRIDE

{% for cuda_version in cuda if not cuda_version.startswith("manifest") %}
{% for distro in cuda[cuda_version] %}
{% for arch in cuda[cuda_version][distro]["arches"] %}
{% set cuda_version_yaml_safe = cuda[cuda_version]["yaml_safe"] %}
{% set distro_yaml_safe = cuda[cuda_version][distro]["yaml_safe"] -%}
{% set image_tag_suffix = cuda[cuda_version][distro]["image_tag_suffix"] -%}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only
  variables:
    ARCH: "{{ arch }}"
    DIST_BASE_PATH: "dist/{{ cuda_version }}"
    MANIFEST: "{{ cuda.manifest_path }}"
    {% if image_tag_suffix %}
    IMAGE_TAG_SUFFIX: {{ image_tag_suffix }}
    {% endif %}
    {% if cuda[cuda_version][distro]["latest"][arch] %}
    LATEST: "true"
    {% endif %}
    {% if cuda[cuda_version][distro]["no_os_suffix"][arch] %}
    NO_OS_SUFFIX: "true"
    {% endif %}
  only:
    variables:
      - ${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }} == "true"
      - $all == "true"

{{ distro }}-v{{ cuda_version }}-{{ arch }}:
  <<: *cuda_base_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only

{% set cudnn_stages = [] %}
{% if cuda[cuda_version][distro]["cudnn"] %}
{% for cudnn in cuda[cuda_version][distro]["cudnn"][arch] -%}

{{ distro }}-v{{ cuda_version }}-{{ cudnn }}-{{ arch }}:
  needs: ["{{ distro }}-v{{ cuda_version }}-{{ arch }}"]
  <<: *cudnn_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only

{% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + cudnn + "-" + arch) %}
{% endfor %}
{% endif -%}

{{ distro }}-v{{ cuda_version }}-test-{{ arch }}:
  needs: ["{{ distro }}-v{{ cuda_version }}-{{ arch }}"]
  <<: *test_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only

{{ distro }}-v{{ cuda_version }}-scan-{{ arch }}:
  needs: ["{{ distro }}-v{{ cuda_version }}-{{ arch }}"]
  <<: *scan_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only

{% set cudnn_stages_final = "" %}
{% if cudnn_stages %}
    {% set cudnn_stages_buf = [] %}
    {% for cudnn_stage in cudnn_stages %}
        {% do cudnn_stages_buf.append('"' + cudnn_stage + '"') %}
    {% endfor %}
    {% set cudnn_stages_final = cudnn_stages_buf|join(",") %}
    {% if cudnn_stages_final != "" %}
        {% set cudnn_stages_final = cudnn_stages_final + ", " %}
    {% endif %}
{% endif -%}

{{ distro }}-v{{ cuda_version }}-deploy-{{ arch }}:
  needs: [{{ cudnn_stages_final }}"{{ distro }}-v{{ cuda_version }}-test-{{ arch }}", "{{ distro }}-v{{ cuda_version }}-scan-{{ arch }}"]
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}_{{ arch }}_only

{% endfor -%}
{% endfor %}
{% endfor %}
