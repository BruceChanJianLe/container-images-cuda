# - DO NOT EDIT - FILE IS AUTOGENERATED

# NOTE: The docker:stable-dind service is not used here because --add-runtime=nvidia does not work from DIND and we need the
#       runtime to perform tests

# Important gitlab-runner considerations
#
# Docker buildx is used for multi-arch images builds. This feature is currently experimental and
# must be explicitly enabled on the docker daemon performing the image builds.
#
# To run multi-arch images on x86_64, qemu-user-static and systemd are required with the following configuration:
#
# $ cat /etc/binfmt.d/qemu-static.conf
# :qemu-aarch64:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\xb7:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff:/usr/bin/qemu-aarch64-static:CF
# :qemu-ppc64le:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x15\x00:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff\x00:/usr/bin/qemu-ppc64le-static:CF
#
# < reboot or start systemd-binfmt.service >
#
# Check with:
#
# $ systemctl status systemd-binfmt.service
# < service should be started >
#
# $ docker run -it nvidia/cuda-ppc64le:11.0-base-ubuntu18.04-rc
# < container should run>


variables:
  # Need a value of two here for checking the manifest in the last commit
  GIT_DEPTH: "2"
  {# DRY_RUN: 1 #}
  {# NO_SCAN: "true" # Only use in development #}
  {# NO_TEST: "true" # Only use in development #}


before_script:
  - 'echo "ARCH: $ARCH"'
  - 'echo "OS: $OS"'
  - 'echo "IMAGE_NAME: $IMAGE_NAME"'
  - 'echo "OS_NAME: $OS_NAME"'
  - 'echo "OS_VERSION: $OS_VERSION"'
  - 'echo "CUDA_VERSION: $CUDA_VERSION"'
  - 'echo "CUDNN_VERSION: $CUDNN_VERSION"'
  - 'echo "IMAGE_TAG_SUFFIX: $IMAGE_TAG_SUFFIX"'
  - 'echo "LATEST: $LATEST"'
  - 'echo "NO_OS_SUFFIX: $NO_OS_SUFFIX"'
  # Gitlab is used to stage images
  - docker login -u "gitlab-ci-token" -p $CI_JOB_TOKEN gitlab-master.nvidia.com:5005
  # Login to NGC to pull images
  - docker login -u '$oauthtoken' -p $NVCR_TOKEN nvcr.io


stages:
  - prepare
  # trigger stage is a workaround until https://gitlab.com/gitlab-org/gitlab-ce/issues/22972 is implemented
  - trigger
  - cuda
  - cudnn
  - test
  - scan
  - deploy
  {# - cleanup #}


# builds the gitlab-cuda-builder image
prepare:
  image: docker:stable
  stage: prepare
  variables:
{% set gitlab_builder_image = cuda["11.0"]["default"]["ubuntu20.04"]["image_name"]["x86_64"] + "/gitlab-cuda-builder" %}
    IMAGE_NAME: "{{ gitlab_builder_image }}"
  script:
    - docker build -t {{ gitlab_builder_image }} --cache-from {{ gitlab_builder_image }} .
    - docker push {{ gitlab_builder_image }}
  tags:
    - docker
  only:
    refs:
      - master
    variables:
      - $TRIGGER == null
      - $TRIGGER_OVERRIDE
    {# changes: #}
    {#   - Dockerfile #}


.tags_template: &tags_definition
  tags:
    - docker


# Only used for CUDA 8.0, will be removed once CUDA 8 support is dropped
.cuda_template_depricated: &cuda_definition_deprecated
  image: {{ gitlab_builder_image }}
  stage: cuda
  retry: 2
  <<: *tags_definition
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel ";
        export CUDA_TAGS="${CUDA_VERSION}-runtime ${CUDA_VERSION}-devel ";
      fi
    - if [[ -z $DIST_BASE_PATH ]]; then
        export DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - docker buildx create --use --platform linux/${ARCH} --driver-opt image=moby/buildkit:master --name cuda-${ARCH}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}" ${TAG_RUNTIME} "${DIST_BASE_PATH}/runtime"
                          --build-arg BUILDKIT_INLINE_CACHE=1
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}" ${TAG_DEVEL} "${DIST_BASE_PATH}/devel"
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}"
                          --build-arg BUILDKIT_INLINE_CACHE=1
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}
    - 'echo "CUDA_TAGS=\"${CUDA_TAGS} ${CUDA_VERSION}-runtime-${OS} ${CUDA_VERSION}-devel-${OS}\"" >> tags.env'
    - cat tags.env
  artifacts:
    reports:
      dotenv: tags.env


.cuda_base_template: &cuda_base_definition
  image: {{ gitlab_builder_image }}
  stage: cuda
  retry: 2
  <<: *tags_definition
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_BASE=" -t ${IMAGE_NAME}:${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} ";
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
        export CUDA_TAGS="${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
      fi
    - if [[ "${LATEST}" == "true" ]]; then
        export TAG_LATEST=" -t ${IMAGE_NAME}:latest ";
        export CUDA_TAGS="$CUDA_TAGS latest";
      fi
    - if [[ -z $DIST_BASE_PATH ]]; then
        export DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - 'echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"'
    - 'echo "TAG_LATEST: ${TAG_LATEST}"'
    - 'echo "TAG_BASE: ${TAG_BASE}"'
    - 'echo "TAG_RUNTIME: ${TAG_RUNTIME}"'
    - 'echo "TAG_DEVEL: ${TAG_DEVEL}"'
    # --driver-opt flag is needed to workaround https://github.com/docker/buildx/issues/386
    - docker buildx create --use --platform linux/${ARCH} --driver-opt image=moby/buildkit:master --name cuda-${ARCH}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_BASE} "${DIST_BASE_PATH}/base"
                          --build-arg BUILDKIT_INLINE_CACHE=1
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_RUNTIME}
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime"
                          --build-arg BUILDKIT_INLINE_CACHE=1
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_DEVEL} ${TAG_LATEST}
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel"
                          --build-arg BUILDKIT_INLINE_CACHE=1
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
    - 'echo "CUDA_TAGS=\"${CUDA_TAGS} ${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}\"" >> tags.env'
    - cat tags.env
  artifacts:
    reports:
      dotenv: tags.env


.cudnn_template: &cudnn_definition
  image: {{ gitlab_builder_image }}
  stage: cudnn
  retry: 2
  <<: *tags_definition
  script:
    - if [[ -z $DIST_BASE_PATH ]]; then
        DIST_BASE_PATH="dist/${OS}/${CUDA_VERSION}/${ARCH}";
      else
        DIST_BASE_PATH="${DIST_BASE_PATH}/${OS}-${ARCH}";
      fi
    - 'echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"'
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        TAG_CUDNN_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
        TAG_CUDNN_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
        export CUDA_EXTRA_TAGS="${CUDA_EXTRA_TAGS} ${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX}";
      fi
    - docker buildx create --use --platform linux/${ARCH} --driver-opt image=moby/buildkit:master --name cuda-${ARCH}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_RUNTIME}
                  --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime/${CUDNN_VERSION}"
                  --build-arg BUILDKIT_INLINE_CACHE=1
                  --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}
    - docker buildx build --pull --push --platform linux/${ARCH} -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_DEVEL}
                  --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel/${CUDNN_VERSION}"
                  --build-arg BUILDKIT_INLINE_CACHE=1
                  --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
    # this is not a great way to pass information between stages, but it is currently the only way
    - 'echo "CUDA_TAGS=${CUDA_TAGS}" > ${CUDNN_VERSION}finaltags.env'
    - 'echo "CUDA_${CUDNN_VERSION}_TAGS=\"${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}\"" >> ${CUDNN_VERSION}finaltags.env'
    - 'echo "CUDA_EXTRA_${CUDNN_VERSION}_TAGS=\"${CUDA_EXTRA_TAGS}\"" >> ${CUDNN_VERSION}finaltags.env'
    - cat ${CUDNN_VERSION}finaltags.env


.test_template: &test_definition
  image: {{ gitlab_builder_image }}
  stage: test
  tags:
    - cuda-test
  script:
    - |
      # NO_TEST SET -- WAR until we have better needs/rules support in the pipelines
      if [[ -z $NO_TEST ]]; then
        bash -e ./test/scripts/bats_install.sh
        bash -e ./test/scripts/run_tests.sh
      else
        exit 0;
      fi


.scan_template: &scan_definition
  image: {{ gitlab_builder_image }}
  stage: scan
  <<: *tags_definition
  script:
    # NO_SCAN SET - WAR until we have better needs/rules support in the pipelines
    - |
      if [[ -z $NO_SCAN ]]; then
        git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/sectooling/scanning/contamer.git
        cd contamer
        pip3 install -r requirements.txt
        docker pull ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}

        # CVE-2020-14352 CVE-2020-15888: librepo, lua-libs packages WAIVED via https://nvbugswb.nvidia.com/NvBugs5/SWBug.aspx?bugid=3028123&cmtNo=35
        python3 contamer.py --debug-logs -ls --fail-on-non-os ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX} \
            --suppress-vulns CVE-2020-14352 CVE-2020-15888 | tee /tmp/output
        retval=$?

        echo "Contamer return value: ${retval}";
        docker rmi -f ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
        if [[ $retval -ne 0 ]]; then
            exit 1;
        else
            # we need to look at the output for "Traceback" because contamer doesn't return non-zero on exception failure...
            if cat /tmp/output | grep -q "Traceback\|Exception: Image analysis failed"; then
                exit 1;
            fi
        fi
      else
        exit 0;
      fi


.deploy_template: &deploy_definition
  image: {{ gitlab_builder_image }}
  stage: deploy
  retry: 2
  <<: *tags_definition
  script:
    - for tag in $(env | grep 'CUDA.*_TAGS' | cut -f2 -d=); do
        {# [[ $tag == "\"\"" ]] && continue; #}
        echo ${tag//\"} >> TAG_MANIFEST;
      done
    - cat TAG_MANIFEST
    - python manager.py ${MANIFEST:+`echo "--manifest ${MANIFEST}"`} push
        --tag-manifest TAG_MANIFEST
        --image-name "${IMAGE_NAME}"
        --os "${OS_NAME}"
        --os-version "${OS_VERSION}"
        --cuda-version "${CUDA_VERSION}"
        --arch "${ARCH}"
        ${PIPELINE_NAME:+`echo "--pipeline-name ${PIPELINE_NAME}"`}
        ${IMAGE_TAG_SUFFIX:+`echo "--tag-suffix ${IMAGE_TAG_SUFFIX}"`} ${DRY_RUN:+"-n"}

trigger:
  image: {{ gitlab_builder_image }}
  stage: trigger
  <<: *tags_definition
  variables:
    MANIFEST: "{{ cuda.manifest_path }}"
  script:
    - echo CI_COMMIT_MESSAGE:$CI_COMMIT_MESSAGE
    - export ESC_TO=`echo $TRIGGER_OVERRIDE | sed 's/\ //g'`
    - export CMD="python manager.py ${MANIFEST:+`echo --manifest ${MANIFEST}`} trigger ${TRIGGER_OVERRIDE:+`echo --trigger-override ${ESC_TO}`}"
    - 'echo "COMMAND: $CMD"'
    - $CMD
  only:
    variables:
      - $TRIGGER == null
      - $TRIGGER_OVERRIDE

{# cleanup: #}
{#   image: {{ gitlab_builder_image }} #}
{#   stage: cleanup #}
{#   <<: *tags_definition #}
{#   script: #}
{#     # Remove all of the images on the host pulled down for test #}
{#     # FIXME: this only works for one runner!! #}
{#     - docker images | grep 'gitlab-master.nvidia.com:5005/cuda-installer' | grep -v "gitlab.*builder" | awk '{ print $3 }' | xargs --no-run-if-empty docker rmi || true #}
{#     [> It' would become difficult to manage all the builder instances for all the possible runners here, so this section is <] #}
{#     [> commented out for now <] #}
{#     [> - docker buildx create --use --platform linux/ppc64le --driver-opt image=moby/buildkit:master --name cuda-ppc64le <] #}
{#     [> - docker buildx stop cuda-ppc64le || true <] #}
{#     [> - docker buildx create --use --platform linux/arm64 --driver-opt image=moby/buildkit:master --name cuda-arm64 <] #}
{#     [> - docker buildx stop cuda-arm64 || true <] #}
{#     [> - docker buildx create --use --platform linux/x86_64 --driver-opt image=moby/buildkit:master --name cuda-x86_64 <] #}
{#     [> - docker buildx stop cuda-x86_64 || true <] #}
{#     - docker system prune -f #}
{#   rules: #}
{#     - if: '$TRIGGER == "true"' #}
{#       when: always #}

{% for cuda_version in cuda if not cuda_version.startswith("manifest") %}
{% for pipeline in cuda[cuda_version] %}
{% for distro in cuda[cuda_version][pipeline] %}
{% for arch in cuda[cuda_version][pipeline][distro]["arches"] -%}

{% set cuda_version_yaml_safe = cuda[cuda_version][pipeline]["yaml_safe"] %}
{% set distro_yaml_safe = cuda[cuda_version][pipeline][distro]["yaml_safe"] -%}
{% set image_tag_suffix = cuda[cuda_version][pipeline][distro]["image_tag_suffix"] -%}
{% set pipeline_name_us = "_" + pipeline if pipeline != "default" else "" -%}
{% set pipeline_name_hyp = "-" + pipeline if pipeline != "default" else "" -%}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables
  ARCH: "{{ arch }}"
  DIST_BASE_PATH: "{{ cuda[cuda_version][pipeline]["dist_base_path"] }}"
  IMAGE_NAME: "{{ cuda[cuda_version][pipeline][distro]["image_name"][arch] }}"
  MANIFEST: "{{ cuda.manifest_path }}"
  {% if image_tag_suffix %}
  IMAGE_TAG_SUFFIX: {{ image_tag_suffix }}
  {% endif %}
  {% if cuda[cuda_version][pipeline][distro]["latest"][arch] %}
  LATEST: "true"
  {% endif %}
  {% if cuda[cuda_version][pipeline][distro]["no_os_suffix"][arch] %}
  NO_OS_SUFFIX: "true"
  {% endif %}
  OS: "{{ distro }}"
  OS_NAME: "{{ cuda[cuda_version][pipeline][distro]["name"] }}"
  OS_VERSION: "{{ cuda[cuda_version][pipeline][distro]["version"] }}"
  CUDA_VERSION: "{{ cuda_version }}"
  {% if pipeline != "default" %}
  PIPELINE_NAME: "{{ pipeline }}"
  {% endif %}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables
  only:
    variables:
      - ${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }} == "true"
      - $all == "true"

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}:
{% if cuda_version == "8.0" %}
  <<: *cuda_definition_deprecated
{% else %}
  <<: *cuda_base_definition
{% endif %}
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only

{% set cudnn_stages = [] %}
{% if cuda[cuda_version][pipeline][distro]["cudnn"] %}
{% for cudnn in cuda[cuda_version][pipeline][distro]["cudnn"][arch] -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ cudnn }}-{{ arch }}:
  <<: *cudnn_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  # Variables overwritten here. Don't move this section.
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_variables
    CUDNN_VERSION: "{{ cudnn }}"
  artifacts:
    reports:
      dotenv: {{ cudnn }}finaltags.env
  needs:
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}
      artifacts: true

{% if pipeline and pipeline != "default" %}
  {% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + pipeline + "-" + cudnn + "-" + arch) %}
{% else %}
  {% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + cudnn + "-" + arch) %}
{% endif %}
{% endfor %}
{% endif -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-test-{{ arch }}:
  needs: ["{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}"]
  <<: *test_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-scan-{{ arch }}:
  needs: ["{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ arch }}", "{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-test-{{ arch }}"]
  <<: *scan_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only

{% set needs = ["{}-v{}{}-{}".format(distro, cuda_version, pipeline_name_hyp, arch), "{}-v{}{}-test-{}".format(distro, cuda_version, pipeline_name_hyp, arch), "{}-v{}{}-scan-{}".format(distro, cuda_version, pipeline_name_hyp, arch)] %}
{% if cudnn_stages %}
    {# {% set needs = [] %} #}
    {% for cudnn_stage in cudnn_stages %}
        {% do needs.append(cudnn_stage) %}
    {% endfor %}
{% endif -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-deploy-{{ arch }}:
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_{{ arch }}_only
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in needs %}
    - job: {{ job }}
    {% if stahp.flag %}
      artifacts: true
      {% do stahp.update({'flag':False}) %}
    {% endif %}
  {% endfor %}

{% endfor -%}
{% endfor %}
{% endfor %}
{% endfor %}
